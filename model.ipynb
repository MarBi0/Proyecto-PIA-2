{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "zLB7aauE0S6s",
        "outputId": "7db21322-7311-4b2e-8090-77267f886f7c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-11-ca2f5073b6e1>:30: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  emociones_augmented.pixels.iloc[n] = transformation(emociones.pixels.iloc[n])\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Exception encountered when calling layer \"feature_extraction_layer\" (type KerasLayer).\n\nin user code:\n\n    File \"/usr/local/lib/python3.10/dist-packages/tensorflow_hub/keras_layer.py\", line 250, in call  *\n        result = smart_cond.smart_cond(training,\n\n    ValueError: Could not find matching concrete function to call loaded from the SavedModel. Got:\n      Positional arguments (3 total):\n        * <tf.Tensor 'inputs:0' shape=(None, 224, 224, 1) dtype=float32>\n        * False\n        * None\n      Keyword arguments: {}\n    \n     Expected these arguments to match one of the following 4 option(s):\n    \n    Option 1:\n      Positional arguments (3 total):\n        * TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name='input_1')\n        * False\n        * None\n      Keyword arguments: {}\n    \n    Option 2:\n      Positional arguments (3 total):\n        * TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name='input_1')\n        * True\n        * None\n      Keyword arguments: {}\n    \n    Option 3:\n      Positional arguments (3 total):\n        * TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name='inputs')\n        * False\n        * None\n      Keyword arguments: {}\n    \n    Option 4:\n      Positional arguments (3 total):\n        * TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name='inputs')\n        * True\n        * None\n      Keyword arguments: {}\n\n\nCall arguments received by layer \"feature_extraction_layer\" (type KerasLayer):\n  • inputs=tf.Tensor(shape=(None, 224, 224, 1), dtype=float32)\n  • training=None",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-ca2f5073b6e1>\u001b[0m in \u001b[0;36m<cell line: 49>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m                                            input_shape=IMAGE_SHAPE+(1,)) # define la forma de la imagen de entrada\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m model = tf.keras.Sequential([\n\u001b[0m\u001b[1;32m     50\u001b[0m     \u001b[0mfeature_extractor_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# utiliza la capa de extracción de características como la base\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCOUNT_UNIQUE_EMOTIONS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'softmax'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'output_layer'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# crea nuestra propia capa de salida\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/trackable/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow_hub/keras_layer.py\u001b[0m in \u001b[0;36mtf__call\u001b[0;34m(self, inputs, training)\u001b[0m\n\u001b[1;32m     72\u001b[0m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msmart_cond\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msmart_cond\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograph_artifact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograph_artifact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUndefined\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'result'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m                 \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mif_stmt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_training_argument\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mif_body_3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melse_body_3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_state_3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mset_state_3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'result'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'training'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m                 \u001b[0;32mdef\u001b[0m \u001b[0mget_state_6\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow_hub/keras_layer.py\u001b[0m in \u001b[0;36melse_body_3\u001b[0;34m()\u001b[0m\n\u001b[1;32m     70\u001b[0m                         \u001b[0mtraining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                     \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mif_stmt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mif_body_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melse_body_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_state_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mset_state_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'training'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msmart_cond\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msmart_cond\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograph_artifact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograph_artifact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUndefined\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'result'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m                 \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mif_stmt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_training_argument\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mif_body_3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melse_body_3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_state_3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mset_state_3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'result'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'training'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow_hub/keras_layer.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     70\u001b[0m                         \u001b[0mtraining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                     \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mif_stmt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mif_body_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melse_body_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_state_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mset_state_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'training'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msmart_cond\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msmart_cond\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograph_artifact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograph_artifact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUndefined\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'result'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m                 \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mif_stmt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_training_argument\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mif_body_3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melse_body_3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_state_3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mset_state_3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'result'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'training'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling layer \"feature_extraction_layer\" (type KerasLayer).\n\nin user code:\n\n    File \"/usr/local/lib/python3.10/dist-packages/tensorflow_hub/keras_layer.py\", line 250, in call  *\n        result = smart_cond.smart_cond(training,\n\n    ValueError: Could not find matching concrete function to call loaded from the SavedModel. Got:\n      Positional arguments (3 total):\n        * <tf.Tensor 'inputs:0' shape=(None, 224, 224, 1) dtype=float32>\n        * False\n        * None\n      Keyword arguments: {}\n    \n     Expected these arguments to match one of the following 4 option(s):\n    \n    Option 1:\n      Positional arguments (3 total):\n        * TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name='input_1')\n        * False\n        * None\n      Keyword arguments: {}\n    \n    Option 2:\n      Positional arguments (3 total):\n        * TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name='input_1')\n        * True\n        * None\n      Keyword arguments: {}\n    \n    Option 3:\n      Positional arguments (3 total):\n        * TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name='inputs')\n        * False\n        * None\n      Keyword arguments: {}\n    \n    Option 4:\n      Positional arguments (3 total):\n        * TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name='inputs')\n        * True\n        * None\n      Keyword arguments: {}\n\n\nCall arguments received by layer \"feature_extraction_layer\" (type KerasLayer):\n  • inputs=tf.Tensor(shape=(None, 224, 224, 1), dtype=float32)\n  • training=None"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import random\n",
        "from scipy import ndimage\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "emociones = pd.read_csv(\"./icml_face_data.csv\", delimiter=\",\")\n",
        "emociones.columns = emociones.columns.str.strip()\n",
        "\n",
        "emociones = emociones.iloc[1:21]\n",
        "emociones.pixels = emociones.pixels.apply(lambda x: np.array(x.split()).reshape(48,48).astype(float))/255\n",
        "\n",
        "\n",
        "def transformation(x):\n",
        "    rotate_rand = random.randint(-30,30)\n",
        "    x = ndimage.rotate(x, rotate_rand, reshape = False)\n",
        "\n",
        "    bright_rand = random.uniform(1.5, 2)\n",
        "    x = np.clip(x * bright_rand, 0.0, 255.0)\n",
        "\n",
        "    return x\n",
        "\n",
        "emociones.pixels = emociones.pixels.apply(lambda x: cv2.resize(x, (224,224)))\n",
        "emociones_augmented = emociones.copy()\n",
        "\n",
        "for n in range(len(emociones_augmented.pixels)):\n",
        "    emociones_augmented.pixels.iloc[n] = transformation(emociones.pixels.iloc[n])\n",
        "\n",
        "\n",
        "result = pd.concat([emociones, emociones_augmented])\n",
        "\n",
        "# plt.imshow(result.iloc[10].pixels, cmap='gray')\n",
        "# plt.show()\n",
        "# plt.imshow(result.iloc[30].pixels, cmap='gray')\n",
        "# plt.show()\n",
        "\n",
        "resnet_url = \"https://www.kaggle.com/models/tensorflow/resnet-50/frameworks/TensorFlow2/variations/feature-vector/versions/1\"\n",
        "IMAGE_SHAPE = (224, 224)\n",
        "COUNT_UNIQUE_EMOTIONS = len(pd.unique(result.emotion))\n",
        "\n",
        "feature_extractor_layer = hub.KerasLayer(resnet_url,\n",
        "                                           trainable=False, # congela los patrones subyacentes\n",
        "                                           name='feature_extraction_layer',\n",
        "                                           input_shape=IMAGE_SHAPE+(1,)) # define la forma de la imagen de entrada\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    feature_extractor_layer, # utiliza la capa de extracción de características como la base\n",
        "    tf.keras.layers.Dense(COUNT_UNIQUE_EMOTIONS, activation='softmax', name='output_layer') # crea nuestra propia capa de salida\n",
        "])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Resnet 50 V2 feature vector\n",
        "resnet_url = \"https://www.kaggle.com/models/tensorflow/resnet-50/frameworks/TensorFlow2/variations/feature-vector/versions/1\"\n",
        "\n",
        "\n",
        "# Original: EfficientNetB0 feature vector (version 1)\n",
        "efficientnet_url=\"https://www.kaggle.com/models/tensorflow/efficientnet/frameworks/TensorFlow2/variations/b0-feature-vector/versions/1\"\n",
        "\n",
        "def create_model(model_url, num_classes=10):\n",
        "  \"\"\"Toma una URL de TensorFlow Hub y crea un modelo secuencial de Keras con ella.\n",
        "\n",
        "  Args:\n",
        "    model_url (str): Una URL de extracción de características de TensorFlow Hub.\n",
        "    num_classes (int): Número de neuronas de salida en la capa de salida,\n",
        "      debe ser igual al número de clases objetivo, por defecto 10.\n",
        "\n",
        "  Returns:\n",
        "    Un modelo secuencial de Keras sin compilar con model_url como capa\n",
        "    de extracción de características y capa de salida densa con num_classes salidas.\n",
        "  \"\"\"\n",
        "  # Descarga el modelo preentrenado y guárdalo como una capa de Keras\n",
        "  feature_extractor_layer = hub.KerasLayer(model_url,\n",
        "                                           trainable=False, # congela los patrones subyacentes\n",
        "                                           name='feature_extraction_layer',\n",
        "                                           input_shape=IMAGE_SHAPE+(3,)) # define la forma de la imagen de entrada\n",
        "\n",
        "  # Crea nuestro propio modelo\n",
        "  model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Flatten(),\n",
        "    feature_extractor_layer, # utiliza la capa de extracción de características como la base\n",
        "    tf.keras.layers.Dense(num_classes, activation='softmax', name='output_layer') # crea nuestra propia capa de salida\n",
        "  ])\n",
        "\n",
        "  return model\n",
        "\n",
        "model = create_model(resnet_url, 4)\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "                     optimizer=tf.keras.optimizers.Adam(),\n",
        "                     metrics=['accuracy'])\n",
        "\n"
      ],
      "metadata": {
        "id": "Ml00uFC42gVd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fotos = result.pixels\n",
        "fotos\n",
        "# model.fit(fotos, epochs=5)\n",
        "# model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JcLqcvwr6zqX",
        "outputId": "f43c7b08-137f-4503-de04-4f50aae61338"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1     [[0.592156862745098, 0.592156862745098, 0.5920...\n",
              "2     [[0.09411764705882353, 0.09411764705882353, 0....\n",
              "3     [[0.0784313725490196, 0.0784313725490196, 0.07...\n",
              "4     [[0.30196078431372547, 0.30196078431372547, 0....\n",
              "5     [[0.3333333333333333, 0.3333333333333333, 0.33...\n",
              "6     [[0.11764705882352941, 0.11764705882352941, 0....\n",
              "7     [[0.01568627450980392, 0.01568627450980392, 0....\n",
              "8     [[0.4196078431372549, 0.4196078431372549, 0.41...\n",
              "9     [[0.054901960784313725, 0.054901960784313725, ...\n",
              "10    [[0.8588235294117647, 0.8588235294117647, 0.85...\n",
              "11    [[0.00392156862745098, 0.00392156862745098, 0....\n",
              "12    [[0.4823529411764706, 0.4823529411764706, 0.48...\n",
              "13    [[0.03137254901960784, 0.03137254901960784, 0....\n",
              "14    [[0.9882352941176471, 0.9882352941176471, 0.98...\n",
              "15    [[0.8784313725490196, 0.8784313725490196, 0.87...\n",
              "16    [[0.6352941176470588, 0.6352941176470588, 0.64...\n",
              "17    [[0.9254901960784314, 0.9254901960784314, 0.92...\n",
              "18    [[0.8235294117647058, 0.8235294117647058, 0.82...\n",
              "19    [[0.19607843137254902, 0.19607843137254902, 0....\n",
              "20    [[0.9176470588235294, 0.9176470588235294, 0.91...\n",
              "1     [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...\n",
              "2     [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...\n",
              "3     [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...\n",
              "4     [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...\n",
              "5     [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...\n",
              "6     [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...\n",
              "7     [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...\n",
              "8     [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...\n",
              "9     [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.120820834631...\n",
              "10    [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...\n",
              "11    [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...\n",
              "12    [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...\n",
              "13    [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...\n",
              "14    [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...\n",
              "15    [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...\n",
              "16    [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...\n",
              "17    [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...\n",
              "18    [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...\n",
              "19    [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...\n",
              "20    [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...\n",
              "Name: pixels, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "#Import from keras_preprocessing not from keras.preprocessing\n",
        "from keras_preprocessing.image import ImageDataGenerator\n",
        "from keras.layers import Dense, Activation, Flatten, Dropout, BatchNormalization\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras import regularizers, optimizers\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "traindf=pd.read_csv(\"./icml_face_data.csv\",dtype=str)\n",
        "datagen=ImageDataGenerator(rescale=1./255.,validation_split=0.25)\n",
        "\n",
        "datagen"
      ],
      "metadata": {
        "id": "1AaHR9sa811m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import random\n",
        "from scipy import ndimage\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "emociones = pd.read_csv(\"./icml_face_data.csv\", delimiter=\",\")\n",
        "emociones.columns = emociones.columns.str.strip()\n",
        "\n",
        "emociones = emociones.iloc[1:21]\n",
        "emociones.pixels = emociones.pixels.apply(lambda x: np.array(x.split()).reshape(48,48).astype(float))/255\n",
        "\n",
        "def transformation(x):\n",
        "    rotate_rand = random.randint(-30,30)\n",
        "    x = ndimage.rotate(x, rotate_rand, reshape = False)\n",
        "\n",
        "    bright_rand = random.uniform(1.5, 2)\n",
        "    x = np.clip(x * bright_rand, 0.0, 255.0)\n",
        "\n",
        "    return x\n",
        "\n",
        "emociones.pixels = emociones.pixels.apply(lambda x: cv2.resize(x, (224,224)))\n",
        "emociones_augmented = emociones.copy()\n",
        "\n",
        "for n in range(len(emociones_augmented.pixels)):\n",
        "    emociones_augmented.pixels.iloc[n] = transformation(emociones.pixels.iloc[n])\n",
        "\n",
        "result = pd.concat([emociones, emociones_augmented])\n",
        "\n",
        "# Convert grayscale images to RGB\n",
        "result['pixels_rgb'] = result['pixels'].apply(lambda x: cv2.cvtColor((x * 255).astype(np.uint8), cv2.COLOR_GRAY2RGB))\n",
        "\n",
        "resnet_url = \"https://www.kaggle.com/models/tensorflow/resnet-50/frameworks/TensorFlow2/variations/feature-vector/versions/1\"\n",
        "IMAGE_SHAPE = (224, 224)\n",
        "COUNT_UNIQUE_EMOTIONS = len(pd.unique(result.emotion))\n",
        "\n",
        "feature_extractor_layer = hub.KerasLayer(resnet_url,\n",
        "                                           trainable=False, # Freeze underlying patterns\n",
        "                                           name='feature_extraction_layer',\n",
        "                                           input_shape=IMAGE_SHAPE+(3,)) # Change input shape to (224, 224, 3)\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    feature_extractor_layer, # Use the feature extraction layer as the base\n",
        "    tf.keras.layers.Dense(COUNT_UNIQUE_EMOTIONS, activation='softmax', name='output_layer') # Create our own output layer\n",
        "])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sDIPFv9sCMRk",
        "outputId": "c13604ea-794d-4af3-9d4f-63e43820d7cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-4385ade50856>:29: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  emociones_augmented.pixels.iloc[n] = transformation(emociones.pixels.iloc[n])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "# Función para crear el modelo ResNet50 con capas adicionales y más regularización\n",
        "def create_model_with_extra_layers_and_regularization(resnet_url, num_classes):\n",
        "    feature_extractor_layer = hub.KerasLayer(resnet_url,\n",
        "                                             trainable=False,\n",
        "                                             name='feature_extraction_layer',\n",
        "                                             input_shape=(224, 224, 3))\n",
        "\n",
        "    model = tf.keras.Sequential([\n",
        "        feature_extractor_layer,\n",
        "        tf.keras.layers.Dense(256, activation='relu'),\n",
        "        tf.keras.layers.Dropout(0.5),  # Aumento de la tasa de Dropout\n",
        "        tf.keras.layers.Dense(128, activation='relu'),\n",
        "        tf.keras.layers.Dropout(0.5),  # Aumento de la tasa de Dropout\n",
        "        tf.keras.layers.Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    return model\n",
        "\n",
        "# Normalización de píxeles\n",
        "X = np.array(result['pixels_rgb'].tolist()) / 255.0\n",
        "\n",
        "# Codificación de etiquetas\n",
        "y = pd.get_dummies(result['emotion']).values\n",
        "\n",
        "# División en conjuntos de entrenamiento y prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Aumento de datos\n",
        "datagen = ImageDataGenerator(rotation_range=30, width_shift_range=0.1, height_shift_range=0.1,\n",
        "                             shear_range=0.2, zoom_range=0.2, horizontal_flip=True, fill_mode='nearest')\n",
        "datagen.fit(X_train)\n",
        "\n",
        "# URL del modelo ResNet50\n",
        "resnet_url = \"https://www.kaggle.com/models/tensorflow/resnet-50/frameworks/TensorFlow2/variations/feature-vector/versions/1\"\n",
        "\n",
        "# Crear el modelo ResNet50 con capas adicionales y más regularización\n",
        "resnet_model = create_model_with_extra_layers_and_regularization(resnet_url, num_classes=y_train.shape[1])\n",
        "\n",
        "# Compilar el modelo\n",
        "resnet_model.compile(loss='categorical_crossentropy',\n",
        "                     optimizer=tf.keras.optimizers.Adam(),\n",
        "                     metrics=['accuracy'])\n",
        "\n",
        "# Entrenar el modelo\n",
        "resnet_history = resnet_model.fit(datagen.flow(X_train, y_train, batch_size=32),\n",
        "                                  epochs=15,\n",
        "                                  steps_per_epoch=len(X_train) // 32,\n",
        "                                  validation_data=(X_test, y_test),\n",
        "                                  callbacks=[create_tensorboard_callback(dir_name=\"tensorflow_hub\",\n",
        "                                                                         experiment_name=\"resnet50V2_with_extra_layers_and_regularization\")])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C115DfH7Dx8p",
        "outputId": "e577827e-cee8-4b05-efd8-fcac401f46f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving TensorBoard log files to: tensorflow_hub/resnet50V2_with_extra_layers_and_regularization/20240310-163836\n",
            "Epoch 1/15\n",
            "1/1 [==============================] - 25s 25s/step - loss: 2.4071 - accuracy: 0.0938 - val_loss: 1.3263 - val_accuracy: 0.2500\n",
            "Epoch 2/15\n",
            "1/1 [==============================] - 11s 11s/step - loss: 3.0217 - accuracy: 0.0938 - val_loss: 1.1391 - val_accuracy: 0.5000\n",
            "Epoch 3/15\n",
            "1/1 [==============================] - 10s 10s/step - loss: 2.0008 - accuracy: 0.3438 - val_loss: 1.1441 - val_accuracy: 0.5000\n",
            "Epoch 4/15\n",
            "1/1 [==============================] - 7s 7s/step - loss: 1.4965 - accuracy: 0.4062 - val_loss: 1.2126 - val_accuracy: 0.5000\n",
            "Epoch 5/15\n",
            "1/1 [==============================] - 10s 10s/step - loss: 1.8652 - accuracy: 0.4375 - val_loss: 1.2683 - val_accuracy: 0.3750\n",
            "Epoch 6/15\n",
            "1/1 [==============================] - 8s 8s/step - loss: 1.7005 - accuracy: 0.3750 - val_loss: 1.2860 - val_accuracy: 0.3750\n",
            "Epoch 7/15\n",
            "1/1 [==============================] - 10s 10s/step - loss: 1.9881 - accuracy: 0.3438 - val_loss: 1.3070 - val_accuracy: 0.3750\n",
            "Epoch 8/15\n",
            "1/1 [==============================] - 10s 10s/step - loss: 1.2948 - accuracy: 0.5312 - val_loss: 1.3564 - val_accuracy: 0.3750\n",
            "Epoch 9/15\n",
            "1/1 [==============================] - 8s 8s/step - loss: 1.7633 - accuracy: 0.3125 - val_loss: 1.3786 - val_accuracy: 0.3750\n",
            "Epoch 10/15\n",
            "1/1 [==============================] - 11s 11s/step - loss: 1.6479 - accuracy: 0.3750 - val_loss: 1.3819 - val_accuracy: 0.3750\n",
            "Epoch 11/15\n",
            "1/1 [==============================] - 9s 9s/step - loss: 1.3332 - accuracy: 0.5625 - val_loss: 1.3667 - val_accuracy: 0.2500\n",
            "Epoch 12/15\n",
            "1/1 [==============================] - 8s 8s/step - loss: 1.7241 - accuracy: 0.3125 - val_loss: 1.3390 - val_accuracy: 0.2500\n",
            "Epoch 13/15\n",
            "1/1 [==============================] - 10s 10s/step - loss: 1.5840 - accuracy: 0.4062 - val_loss: 1.3303 - val_accuracy: 0.2500\n",
            "Epoch 14/15\n",
            "1/1 [==============================] - 9s 9s/step - loss: 1.3290 - accuracy: 0.3750 - val_loss: 1.3403 - val_accuracy: 0.2500\n",
            "Epoch 15/15\n",
            "1/1 [==============================] - 9s 9s/step - loss: 1.3746 - accuracy: 0.5000 - val_loss: 1.3205 - val_accuracy: 0.3750\n"
          ]
        }
      ]
    }
  ]
}